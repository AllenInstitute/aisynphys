import functools, os, re, pickle
import numpy as np
import aisynphys.config


def model_result_cache_path():
    return os.path.join(aisynphys.config.cache_path, 'stochastic_model_results')


def list_cached_results(cache_path=None):
    """Return a list of (pair_id, cache_file) for all cached model results.
    """
    if cache_path is None:
        cache_path = model_result_cache_path()
    files = os.listdir(cache_path)
    results = []
    for cf in files:
        if re.match(r'[0-9]{10}\.[0-9]{3}_[0-9]_[0-9].pkl$', cf) is None:
            continue
        syn_id = tuple(os.path.splitext(cf)[0].split('_'))
        results.append((syn_id, os.path.join(cache_path, cf)))
    return results


def load_cache_file(cache_file, db):
    """Load cached result and return a StochasticModelRunner instance for a single synapse model run.
    """
    from .model_runner import StochasticModelRunner
    fn = os.path.splitext(os.path.split(cache_file)[1])[0]
    expt_id, pre_cell_id, post_cell_id = fn.split('_')
    mr = StochasticModelRunner(db, expt_id, pre_cell_id, post_cell_id)
    mr.load_result(cache_file)
    if isinstance(mr.param_space, list):
        raise Exception(mr.param_space[-1])
    return mr


def load_cached_model_results(cache_files, db, mmap_file=None):
    """Return cached model results from multiple synapses.

    Parameters
    ----------
    cache_files : list
        List of cache files containing model results (generated by StochasticModelRunner.store_result).
        All files must contain results generated using the same parameter space.
    mmap_file : str | None
        Name of a file in which to store result data, which will be returned as a memory-mapped array.
        If None, then the dataset is loaded into memory instead.

    Returns
    -------
    results : ndarray
        Array of aggregated results; the first axis index corresponds to each file loaded
    cache_files : list
        List of file names loaded. May differ from the input list if some files encountered errors.
    param_space : dict
        Structure describing the modeled parameter space.
    """
    shape = None
    results = None
    mask = np.zeros(len(cache_files), dtype=bool)

    from aisynphys.ui.progressbar import ProgressBar
    with ProgressBar("Loading model results", len(mask)) as prg:
        ptr = 0
        for i,fn in enumerate(cache_files):
            prg.update(i+1, "%d / %d: %s" % (i+1, prg.maximum, fn))

            try:
                mr = load_cache_file(fn, db)
            except Exception as exc:
                raise
                print("Error; ignoring cache file %s: %s" % (fn, str(exc)))
                continue

            if results is None:
                shape = (len(cache_files),) + mr.param_space.result.shape + (2,)
                try:
                    if mmap_file is not None:
                        results = np.memmap(open(mmap_file, 'w+b'), dtype='float32', shape=shape)
                    else:
                        results = np.empty(shape, dtype='float32')
                except MemoryError:
                    raise MemoryError("Failed allocation: %0.3fGB" % (np.product(shape) * 4 / 1e9))

            results[ptr, ..., 0] = mr.param_space.result['likelihood']
            results[ptr, ..., 1] = mr.param_space.result['mini_amplitude']
            ptr += 1
            mask[i] = True

    results = results[:ptr]
    cache_files = np.array(cache_files)[mask]
    
    return results, cache_files, mr.param_space.axes()


@functools.lru_cache(maxsize=1)
def load_spca_results(max_vector_size=None):
    """Return a dict containing results of sparse PCA run on the posterior distribution of likelihood
    values across model parameters.

    Results are originally generated by aisynphys/analyses/stochastic_model_reduction.py and stored
    in the location specified by config.release_model_spca_file.

    Contains:
    - model: sklearn sparse PCA model
    - param_space: release model

    """
    model_file = aisynphys.config.release_model_spca_file

    sm_results = pickle.load(open(model_file, 'rb'))

    results = {
        'model': sm_results['sparse_pca'],
        'param_space': sm_results['params'],
        'synapse_vectors': {},
    }
    for i, cache_file in enumerate(sm_results['cache_files']):
        expt_id, pre_cell_id, post_cell_id = os.path.split(os.path.splitext(cache_file)[0])[1].split('_')
        syn_id = (expt_id, pre_cell_id, post_cell_id)
        vector = sm_results['result'][i]
        if max_vector_size is not None:
            vector = vector[:max_vector_size]
        results['synapse_vectors'][syn_id] = vector

    return results
